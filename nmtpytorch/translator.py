import sys
import time
from pathlib import Path

from .utils.misc import load_pt_file
from .utils.filterchain import FilterChain

from nmtpytorch import logger
from nmtpytorch import models
from .config import Options


class Translator(object):
    """A utility class to pack translation related features."""

    def __init__(self, **kwargs):
        # Setup logger
        self.logger = logger.setup(None, 'translate')

        # Store attributes directly. See bin/nmtpy for their list.
        self.__dict__.update(kwargs)

        if self.source and not self.output:
            self.logger.info(
                'Error: Output file should be given when using -S.')
            sys.exit(1)

        # How many models?
        self.n_models = len(self.models)

        # Print some information
        self.logger.info('{} model(s) - beam_size: {}, batch_size: {}'.format(
            self.n_models, self.beam_size, self.batch_size))

        self.instances = []

        # NOTE: Do some sanity checks here to make sure that models
        # are compatible for ensembling
        for model_file in self.models:
            weights, _, opts = load_pt_file(model_file)
            opts = Options.from_dict(opts)
            # Create model instance
            instance = getattr(models, opts.train['model_type'])(
                opts=opts, logger=self.logger)
            # Setup layers
            instance.setup()
            # Load weights
            instance.load_state_dict(weights, strict=True)
            # Move to GPU
            instance.cuda()
            # Switch to eval mode
            instance.train(False)
            self.instances.append(instance)

        # NOTE: Remove this once ensembling is ready
        self.instance = self.instances[0]

        if self.disable_filters or not self.instance.opts.train['eval_filters']:
            self.logger.info('Post-processing filters disabled.')
            self.filter = lambda s: s
        else:
            self.logger.info('Post-processing filters enabled.')
            self.filter = FilterChain(self.instance.opts.train['eval_filters'])

        # Can be a comma separated list of hardcoded test splits
        if self.splits:
            self.logger.info('Will translate "{}"'.format(self.splits))
            self.inputs = self.splits.split(',')
        elif self.source:
            self.logger.info('Will translate "{}"'.format(self.source))
            # Hack to make it like the given file is the split 'new'
            self.instance.opts.data['new_set'] = {
                instance.sl: Path(self.source)}
            self.inputs = ['new']

    def translate(self, instance, split):
        """Returns the hypotheses generated by translating the given split
        using the given model instance.

        Arguments:
            instance(nn.Module): An initialized nmtpytorch model instance.
            split(str): A test split defined in the .conf file before
                training.

        Returns:
            list:
                A list of optionally post-processed string hypotheses.
        """

        instance.load_data(split)
        loader = instance.datasets[split].get_iterator(
            self.batch_size, only_source=True)
        self.logger.info('Starting translation')

        start = time.time()
        if self.beam_size == 1 and hasattr(instance, 'greedy_search'):
            hyps = instance.greedy_search(loader, self.max_len,
                                          self.avoid_double, self.avoid_unk)
        else:
            hyps = instance.beam_search(loader, self.beam_size, self.max_len,
                                        self.avoid_double, self.avoid_unk)

        up_time = time.time() - start
        self.logger.info('Took {:.3f} seconds, {:.4f} sec/hyp'.format(
            up_time, up_time / len(hyps)))

        return self.filter(hyps)

    def dump(self, hyps, split, output):
        """Writes the results into output.

        Arguments:
            hyps(list): A list of hypotheses.
        """
        if not output:
            output = "{}.{}.beam{}".format(
                self.models[0], split, self.beam_size)

        with open(output, 'w') as f:
            for line in hyps:
                f.write(line + '\n')

    def __call__(self):
        """Dumps the hypotheses for each of the requested split/file."""
        for input_ in self.inputs:
            hyps = self.translate(self.instance, input_)
            self.dump(hyps, input_, self.output)
