#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import platform
import argparse

import numpy as np
import torch

from nmtpytorch.config import Options
from nmtpytorch import logger
from nmtpytorch.utils.misc import setup_experiment, load_pt_file
from nmtpytorch.utils.gpu import GPUManager
from nmtpytorch.translator import Translator
from nmtpytorch import models
from nmtpytorch.mainloop import MainLoop


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        prog='nmtpy',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description="nmtpy trains/resumes/translates a given "
                    "configuration/checkpoint/snapshot",
        argument_default=argparse.SUPPRESS)

    subparsers = parser.add_subparsers(dest='cmd', title='sub-commands',
                                       description='Valid sub-commands')

    # train command
    parser_train = subparsers.add_parser('train', help='train help')
    parser_train.add_argument('-C', '--config', type=str, required=True,
                              help="Experiment configuration file")
    parser_train.add_argument('-s', '--suffix', type=str, default="",
                              help="Optional experiment suffix.")
    parser_train.add_argument('overrides', nargs="*", default=[],
                              help="(section).key:value overrides for config")

    # resume command
    parser_resume = subparsers.add_parser('resume', help='resume help')
    parser_resume.add_argument('checkpoint', type=str,
                               help="Checkpoint (.ckpt) file for resuming")

    # translate command
    parser_trans = subparsers.add_parser('translate', help='translate help')
    parser_trans.add_argument('-n', '--disable-filters', action='store_true',
                              help='Disable eval_filters given in config')
    parser_trans.add_argument('-b', '--batch-size', type=int, default=16,
                              help='Batch size for beam-search')
    parser_trans.add_argument('-k', '--beam-size', type=int, default=6,
                              help='Beam size for beam-search')
    parser_trans.add_argument('-m', '--max-len', type=int, default=100,
                              help='Maximum sequence length')
    parser_trans.add_argument('-p', '--avoid-double', action='store_true',
                              help='Suppress previous token probs')
    parser_trans.add_argument('-u', '--avoid-unk', action='store_true',
                              help='Suppress <unk> generation')
    parser_trans.add_argument('-d', '--device-id', type=str, default='auto_1',
                              help='Select GPU device(s)')
    parser_trans.add_argument('models', type=str, nargs='+',
                              help="Saved model/checkpoint file(s)")

    group = parser_trans.add_mutually_exclusive_group(required=True)
    # You can translate a set of splits defined in the .conf file
    # In this case, outputs will be automatically <model_file>.<split>.<beam>
    group.add_argument('-s', '--splits', type=str,
                       help='Comma separated splits from config file')
    group.add_argument('-S', '--source', type=str,
                       help='A text file to translate (pass - for stdin)')
    parser_trans.add_argument('-o', '--output', type=str,
                              help='Explicit output file if using -S <input>')

    # Parse command-line arguments first
    args = parser.parse_args()
    if args.cmd is None:
        parser.print_help()
        sys.exit(1)

    opts, history, weights = {}, {}, None

    # Mode selection
    if args.cmd == 'train':
        # Parse configuration file and merge with the rest
        opts = Options(args.config, args.overrides)

        # Setup experiment folders
        setup_experiment(opts, args.suffix)

    elif args.cmd == 'resume':
        # Load everything to CPU without messing with storage tags
        weights, history, opts = load_pt_file(args.checkpoint)
        opts = Options.from_dict(opts)

    # Detect device_id
    device_id = opts.train['device_id'] if opts else args.device_id

    # Reserve GPUs
    gpu_devs = GPUManager()(device_id, strict=True)
    assert len(gpu_devs) == 1, "Multi-GPU mode not implemented yet."

    #######################
    # translate entry point
    #######################
    if args.cmd == 'translate':
        assert len(args.models) == 1, "Ensembling not implemented yet."
        Translator(**args.__dict__)()
        sys.exit(0)

    #################################
    # Training / Resuming entry point
    #################################
    log = logger.setup(opts.train, args.cmd)

    # Instantiate the model object
    model = getattr(models, opts.train['model_type'])(opts=opts, logger=log)

    if opts.train['seed'] != 0:
        np.random.seed(opts.train['seed'])
        torch.manual_seed(opts.train['seed'])

    # Be verbose and fire the loop!
    opts.info(log)
    log.info("PyTorch {} (CUDA: {}) on '{}' (GPUs: {})".format(
        torch.__version__, torch.version.cuda, platform.node(), gpu_devs))
    loop = MainLoop(model, log, opts.train, history, weights, mode=args.cmd)
    loop.run()
    sys.exit(0)
